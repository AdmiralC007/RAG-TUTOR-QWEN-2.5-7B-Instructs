ğŸ“˜ RAG Tutor â€“ Chunking, Retrieval & Tutor (Streamlit)
======================================================

A **Retrieval-Augmented Generation (RAG) Tutor** that allows users to upload PDF documents and ask questions grounded strictly in the document content.The system uses **semantic search + LLM reasoning** to behave like a tutor rather than a chatbot.

ğŸš€ Project Overview
-------------------

Traditional LLMs hallucinate because they rely only on pre-trained knowledge.This project solves that by implementing a **RAG pipeline** where:

1.  Documents are **chunked**
    
2.  Converted into **vector embeddings**
    
3.  Stored in a **FAISS vector database**
    
4.  Relevant context is retrieved for each query
    
5.  An LLM generates **grounded, tutor-style answers**
    

If the answer is **not present in the document**, the model explicitly says so.

ğŸ§  Key Features
---------------

*   ğŸ“„ Upload any PDF document
    
*   âœ‚ï¸ Automatic text chunking with metadata
    
*   ğŸ” Semantic similarity search using FAISS
    
*   ğŸ§  Tutor-style answers (not raw chunk dumps)
    
*   ğŸš« Hallucination control via strict prompting
    
*   â³ Rate-limit aware (Hugging Face free tier safe)
    
*   ğŸ” Clear vectorstore & upload new documents
    
*   â˜ï¸ Ready for Streamlit Community Cloud deployment
    

ğŸ—ï¸ Architecture (High Level)
-----------------------------

```
PDF Upload
   â†“
PDF Loader
   â†“
Text Chunking
   â†“
Embedding Model
   â†“
FAISS Vector Store
   â†“
Similarity Search (Top-K Chunks)
   â†“
Prompt + Context
   â†“
LLM (Qwen / Mixtral via HF)
   â†“
Tutor-Style Answer

```

ğŸ› ï¸ Tech Stack
--------------

### Frontend

*   **Streamlit** â€“ UI and interaction layer
    

### Backend / ML

*   **LangChain (v1)** â€“ RAG orchestration
    
*   **FAISS** â€“ Vector similarity search
    
*   **Sentence Transformers** â€“ Embeddings
    
*   **Hugging Face Inference API** â€“ Free LLM inference
    
*   **Qwen2.5 / Mixtral** â€“ Instruction-tuned LLMs
    

### Utilities

*   **Python**
    
*   **dotenv** â€“ Environment variable handling
    

ğŸ“‚ Project Structure
--------------------
```
rag-tutor-streamlit/
â”‚
â”œâ”€â”€ app.py                     # Main Streamlit app
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ README.md                  # Documentation
â”œâ”€â”€ .gitignore                 # Ignored files
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ loader.py              # PDF loading logic
â”‚   â”œâ”€â”€ splitter.py            # Chunking strategy
â”‚   â”œâ”€â”€ embeddings.py          # Embedding model
â”‚   â”œâ”€â”€ vectorstore.py         # FAISS vector store
â”‚   â”œâ”€â”€ llm.py                 # Hugging Face LLM setup
â”‚   â”œâ”€â”€ prompt.py              # Strict RAG prompt
â”‚   â”œâ”€â”€ chain.py               # Context formatting
â”‚   â””â”€â”€ utils.py               # Helper utilities
â”‚
â””â”€â”€ data/                      # Uploaded PDFs (runtime)

```


ğŸ§  RAG Prompt Strategy (Hallucination Control)
----------------------------------------------

The model is instructed to:

*   Answer **only using retrieved context**
    
*   _"I don't know based on the provided document"_if the answer is not found
    

This ensures:

*   No fabricated answers
    
*   High factual accuracy
    
*   Trustworthy responses
    

â³ Rate Limiting & Reliability
-----------------------------

Because the project uses **Hugging Face Free Tier**, the following safeguards are implemented:

*   UI-level throttling (1 request per 5 seconds)
    
*   Token limits (max\_new\_tokens)
    
*   Graceful fallback when the model is busy
    
*   Retrieval results shown even if LLM is unavailable
    

This makes the system **robust and production-aware**.

ğŸ” Environment Variables
------------------------

The project requires a Hugging Face API token.

Create a .env file locally:

`   HUGGINGFACEHUB_API_TOKEN=your_token_here   `

> âš ï¸ .env is excluded via .gitignore and never committed.

â–¶ï¸ Running the Project Locally
------------------------------

### 1ï¸âƒ£ Create virtual environment
```
python -m venv venv  source venv/bin/activate  
# Windows: venv\Scripts\activate   
```

### 2ï¸âƒ£ Install dependencies

`   pip install -r requirements.txt   `

### 3ï¸âƒ£ Run Streamlit app

`   streamlit run app.py   `

ğŸ§ª Example Use Case
-------------------

1.  Upload a dictionary / textbook / notes PDF
    
2.  Define apple in this document
    
3.  The tutor:
    
    *   Retrieves relevant chunks
        
    *   Generates a grounded explanation
        
    *   Avoids hallucinations
        

ğŸ“ˆ Why This Project Matters
---------------------------

This project demonstrates:

*   Real-world **RAG architecture**
    
*   Modern **LangChain v1** usage
    
*   Vector database lifecycle management
    
*   LLM rate-limit handling
    
*   Clean Streamlit engineering
    
*   Production-aware ML system design
    

ğŸ§  Interview-Ready Talking Points
---------------------------------

*   â€œI separated retrieval from generation to prevent hallucination.â€
    
*   â€œVector stores are treated as runtime databases, not version-controlled artifacts.â€
    
*   â€œI implemented UI-level throttling to handle LLM rate limits gracefully.â€
    
*   â€œThe system remains useful even when the LLM is unavailable.â€
    

ğŸš€ Future Improvements
----------------------

*   Multi-PDF support
    
*   Chat history memory
    
*   Source citation per answer
    
*   Async retrieval
    
*   Alternative vector stores (Chroma / Milvus)
    
*   Deployment on Streamlit Community Cloud
    

ğŸ“œ License
----------

This project is licensed under the **Apache 2.0 License**.

ğŸ™Œ Author
---------

**Maddikatla Chaitanya**B.E Undergraduate | Full-Stack & ML Enthusiast
